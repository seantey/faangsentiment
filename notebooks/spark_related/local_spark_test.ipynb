{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Code Development Notebook\n",
    "* Version: 1.0.0\n",
    "* Last updated: Jul 19, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used for development and demonstration of PySpark code to process the news articles for a given analysis window and then write the resulting sentiment analysis scores and labels back to DynamoDB.\n",
    "\n",
    "V1.0.0 is testing the pipeline with only looking at news_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data from DynamoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3.dynamodb.conditions import Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'test_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamodb = boto3.resource('dynamodb')\n",
    "table = dynamodb.Table(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab data from a particular analysis window\n",
    "target_analysis_window = '2020-07-15_Hour=17'\n",
    "\n",
    "response = table.query(\n",
    "    KeyConditionExpression=Key('analysis_window').eq(target_analysis_window)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a list of dictionaries where\n",
    "# the keys of each dictionary is a key/column in the DynamoDB table\n",
    "news_data = response['Items']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start spark and parallelize the data for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "                        .appName(\"SparkTest\") # Set app name\n",
    "                        .master(\"local[2]\") # Run locally with 2 cores\n",
    "                        .config(\"spark.driver.memory\", \"4g\")\n",
    "                        .config(\"spark.executor.memory\", \"3g\")\n",
    "                        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean-cx1/anaconda3/lib/python3.7/site-packages/pyspark/sql/session.py:366: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- analysis_window: string (nullable = true)\n",
      " |-- api_success_utc_str: string (nullable = true)\n",
      " |-- api_success_utc_ts: decimal(38,18) (nullable = true)\n",
      " |-- news_content: string (nullable = true)\n",
      " |-- news_link: string (nullable = true)\n",
      " |-- news_publisher: string (nullable = true)\n",
      " |-- news_timestamp: decimal(38,18) (nullable = true)\n",
      " |-- news_title: string (nullable = true)\n",
      " |-- source_api: string (nullable = true)\n",
      " |-- symb_id_source: string (nullable = true)\n",
      " |-- t_symb: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This method throws deprecated function warning\n",
    "#Convert list to RDD\n",
    "news_rdd = spark.sparkContext.parallelize(news_data)\n",
    "#Create data frame\n",
    "news_df = spark.createDataFrame(news_rdd)\n",
    "\n",
    "# Alternative method which doesn't throw deprecated warning:\n",
    "# https://kontext.tech/column/spark/366/convert-python-dictionary-list-to-pyspark-dataframe\n",
    "# from pyspark.sql import Row\n",
    "# news_df = spark.createDataFrame([Row(**i) for i in news_data])\n",
    "# BUT! This method throws an error with .rdd.flatMap(lambda x: x).collect()\n",
    "# java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. \n",
    "# 11 fields are required while 10 values are provided.\n",
    "# However, we don't usually collect until the end of the pipeline.\n",
    "\n",
    "news_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apple News adds new audio features, including a daily briefing, alongside expanded local coverage'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data[0]['news_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls, most likely there won't be any for titles\n",
    "title_df = news_df.select('analysis_window', 't_symb', 'news_timestamp', 'news_title').na.drop()\n",
    "# https://stackoverflow.com/questions/38610559/\n",
    "title_list = title_df.rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "from multiprocessing import cpu_count\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisPipeline:\n",
    "    def __init__(self):\n",
    "        # Note that the transformers pipelines can take list of strings as input\n",
    "        # instead of just one string.\n",
    "#         self.summarizer_pipeline = pipeline(\"summarization\")\n",
    "        self.sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "        \n",
    "        # multiprocessing core count heuristic from \n",
    "        # comment in https://stackoverflow.com/questions/20886565/\n",
    "        self.pool_cores = cpu_count()-1 or 1\n",
    "        \n",
    "    def strip_html(self,input_string):\n",
    "        \"\"\"\n",
    "        Strips any HTML tags from a string\n",
    "        \"\"\" \n",
    "        \n",
    "        cleaned_text = BeautifulSoup(input_string).text\n",
    "        \n",
    "        return cleaned_text\n",
    "    \n",
    "    def strip_html_multi(self, input_string_list):\n",
    "                \n",
    "        # Make sure input is a list, or if it's one string\n",
    "        # convert to list.\n",
    "        if type(input_string_list) is not list:\n",
    "            input_string_list = list(input_string_list)     \n",
    "\n",
    "        # TODO may want to initialize and destroy pool somwhere else\n",
    "        # Or it might not matter too much given this function is only\n",
    "        # called once per spark job / AWS EMR startup\n",
    "        p = 2\n",
    "        with Pool(processes=p) as pool:\n",
    "            chunksize = 3\n",
    "            no_html_text = pool.map(self.strip_html, input_string_list, chunksize)        \n",
    "        \n",
    "        return no_html_text\n",
    "    \n",
    "    def raw_text_to_sentiment(self,input_string):\n",
    "        \"\"\"\n",
    "        Takes a list of strings or just one regular string and runs it\n",
    "        through a pipeline to get the positive/negative label and the\n",
    "        respective scores.\n",
    "        \n",
    "        Pipeline consists of:\n",
    "        (1) Removing HTML if present\n",
    "        (2) Summarizing the news articles\n",
    "        (3) Calculating a sentiment score and label (Positive or Negative)\n",
    "        \n",
    "        Returns a list of dictionaries with label and score as keys.\n",
    "        \"\"\"\n",
    "\n",
    "#         no_html_text = self.strip_html_multi(input_string)\n",
    "        \n",
    "#         news_summary = self.summarizer_pipeline(no_html_text, \n",
    "#                                                 max_length=300, \n",
    "#                                                 min_length=30)[0]['summary_text']\n",
    "#         sentiment_scores = self.sentiment_pipeline(summary_news)\n",
    "\n",
    "        # TODO check for token length\n",
    "\n",
    "        # Temporary version expects news titles only\n",
    "        label = self.sentiment_pipeline(input_string)[0]['label'] \n",
    "        score = self.sentiment_pipeline(input_string)[0]['score']\n",
    "\n",
    "        return (label, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pipe = SentimentAnalysisPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-07-15_Hour=17'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = s_pipe.raw_text_to_sentiment(title_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NEGATIVE', 0.6570926308631897)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try adding a UDF for transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_schema = StructType([\n",
    "    StructField(\"label\", StringType(), nullable=False),\n",
    "    StructField(\"score\", FloatType(), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pipe_udf = udf(s_pipe.raw_text_to_sentiment,udf_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[analysis_window: string, t_symb: string, news_timestamp: decimal(38,18), news_title: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+--------------------+--------------------+\n",
      "|   analysis_window|t_symb|      news_timestamp|          news_title|\n",
      "+------------------+------+--------------------+--------------------+\n",
      "|2020-07-15_Hour=17|  AAPL|1594832503.000000...|Apple News adds n...|\n",
      "|2020-07-15_Hour=17|  AAPL|1594826311.000000...|Over 2,500 games ...|\n",
      "|2020-07-15_Hour=17|  AAPL|1594859001.000000...|Dow Jones Futures...|\n",
      "|2020-07-15_Hour=17|  AAPL|1594851916.000000...|Twitter Hack Snag...|\n",
      "|2020-07-15_Hour=17|  AAPL|1594850401.000000...|Apple Ruling Make...|\n",
      "+------------------+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = title_df.withColumn('sentiment', s_pipe_udf(title_df['news_title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- analysis_window: string (nullable = true)\n",
      " |-- t_symb: string (nullable = true)\n",
      " |-- news_timestamp: decimal(38,18) (nullable = true)\n",
      " |-- news_title: string (nullable = true)\n",
      " |-- sentiment: struct (nullable = true)\n",
      " |    |-- label: string (nullable = false)\n",
      " |    |-- score: float (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = sentiment_df.select('analysis_window', 't_symb', 'news_timestamp',\n",
    "                                   'news_title', 'sentiment.label', 'sentiment.score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- analysis_window: string (nullable = true)\n",
      " |-- t_symb: string (nullable = true)\n",
      " |-- news_timestamp: decimal(38,18) (nullable = true)\n",
      " |-- news_title: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- score: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+--------------------+--------------------+--------+----------+\n",
      "|   analysis_window|t_symb|      news_timestamp|          news_title|   label|     score|\n",
      "+------------------+------+--------------------+--------------------+--------+----------+\n",
      "|2020-07-15_Hour=17|  AAPL|1594832503.000000...|Apple News adds n...|POSITIVE| 0.9947432|\n",
      "|2020-07-15_Hour=17|  AAPL|1594826311.000000...|Over 2,500 games ...|NEGATIVE| 0.9982638|\n",
      "|2020-07-15_Hour=17|  AAPL|1594859001.000000...|Dow Jones Futures...|NEGATIVE|0.99574244|\n",
      "|2020-07-15_Hour=17|  AAPL|1594851916.000000...|Twitter Hack Snag...|NEGATIVE|0.99829704|\n",
      "|2020-07-15_Hour=17|  AAPL|1594850401.000000...|Apple Ruling Make...|NEGATIVE|0.99890846|\n",
      "|2020-07-15_Hour=17|  AAPL|1594847766.000000...|TikTok’s Huge Dat...|NEGATIVE|  0.968786|\n",
      "|2020-07-15_Hour=17|  AAPL|1594847280.000000...|Hackers compromis...|NEGATIVE| 0.9991414|\n",
      "|2020-07-15_Hour=17|  AAPL|1594844640.000000...|Is Tesla Worth $2...|NEGATIVE|0.98299336|\n",
      "|2020-07-15_Hour=17|  AAPL|1594839060.000000...|Investing legend ...|POSITIVE|  0.993847|\n",
      "|2020-07-15_Hour=17|  AAPL|1594837935.000000...|Is TikTok a secur...|NEGATIVE| 0.9969733|\n",
      "+------------------+------+--------------------+--------------------+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Label Calculation:\n",
    "\n",
    "We want a final label of either **POSITIVE**, **NEGATIVE**, or **UNCERTAIN**. \n",
    "\n",
    "We will use a somewhat naive and simple approach to calculating sentiment through averaging.\n",
    "\n",
    "Criteria for final score:\n",
    "* The final score should be between -1 and 1.\n",
    "* The older news, the less important it is, scores are weighed exponentially less every 3 hours from the most\n",
    "* recent news. E.g. Most recent news have a weight of 1 and news that are 3 hours away from the MAX timestamp have a weight of 0.5, 6 hour away from MAX timestamp has weight of 0.25 and so on.\n",
    "* Any score between -0.7 and 0.7 (exclusive) is labelled UNCERTAIN\n",
    "* Scores that are 0.7 or greater are labelled POSITIVE\n",
    "* Scores that are -0.7 or less are labelled NEGATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, floor as spark_floor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores are bounded between -1 and 1\n",
    "If the label is NEGATIVE, make the score value negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = sentiment_df.withColumn('score', \n",
    "                                       (when(sentiment_df.label == 'NEGATIVE', -sentiment_df.score)\n",
    "                                        .otherwise(sentiment_df.score))\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+--------------------+--------------------+--------+-----------+\n",
      "|   analysis_window|t_symb|      news_timestamp|          news_title|   label|      score|\n",
      "+------------------+------+--------------------+--------------------+--------+-----------+\n",
      "|2020-07-15_Hour=17|  AAPL|1594832503.000000...|Apple News adds n...|POSITIVE|  0.9947432|\n",
      "|2020-07-15_Hour=17|  AAPL|1594826311.000000...|Over 2,500 games ...|NEGATIVE| -0.9982638|\n",
      "|2020-07-15_Hour=17|  AAPL|1594859001.000000...|Dow Jones Futures...|NEGATIVE|-0.99574244|\n",
      "|2020-07-15_Hour=17|  AAPL|1594851916.000000...|Twitter Hack Snag...|NEGATIVE|-0.99829704|\n",
      "|2020-07-15_Hour=17|  AAPL|1594850401.000000...|Apple Ruling Make...|NEGATIVE|-0.99890846|\n",
      "|2020-07-15_Hour=17|  AAPL|1594847766.000000...|TikTok’s Huge Dat...|NEGATIVE|  -0.968786|\n",
      "|2020-07-15_Hour=17|  AAPL|1594847280.000000...|Hackers compromis...|NEGATIVE| -0.9991414|\n",
      "|2020-07-15_Hour=17|  AAPL|1594844640.000000...|Is Tesla Worth $2...|NEGATIVE|-0.98299336|\n",
      "|2020-07-15_Hour=17|  AAPL|1594839060.000000...|Investing legend ...|POSITIVE|   0.993847|\n",
      "|2020-07-15_Hour=17|  AAPL|1594837935.000000...|Is TikTok a secur...|NEGATIVE| -0.9969733|\n",
      "+------------------+------+--------------------+--------------------+--------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old news weigh less\n",
    "\n",
    "The older the news, the less important it is, scores are weighted exponentially less every 3 hours from the most recent timestamp in the analysis window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import max as spark_max\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side note: Remember that Spark Dataframes are based on RDDs which are immutable, \n",
    "# instead the steps are all lazily evaluated so that process can be optimized by Spark.\n",
    "# Also note that we are calling .show() for demo/debug purposes only as this causes Spark\n",
    "# to execute and collect everything onto the Driver node.\n",
    "\n",
    "# Calculate weight factor\n",
    "# Since we want the weight to be halved every 3 hours, the weight is basically\n",
    "# 1 / (2^h) where h is the hours away from max divided by 3 and rounded down [h = floor(diff_hour/3)]\n",
    "# e.g. 5 Hour difference from MAX timestamp means h = floor(5/3) and weight = 1/(2^1) = 1/2\n",
    "\n",
    "# Spark transformations needed:\n",
    "# 1. Get most latest (max) timestamp of news articles in each analysis window and stock\n",
    "# 2. Convert news_timestamp which is seconds from epoch to hour from epoch.\n",
    "# 3. Calculate the number of hour difference between the current row value and max value\n",
    "#    in terms of news_timestamp hours from epoch.\n",
    "# 4. Divide this difference by 3 and get the floor\n",
    "# 5. Calculate the weight which is 1/(2^h) where h = floor(diff_hour/3), h was calculated in step (3)\n",
    "# 6. Multiply the sentiment score by the weight to get the new time weighted score column\n",
    "\n",
    "\n",
    "# 1. Get max timestamp (epoch seconds) for each analysis window and stock ticker\n",
    "# https://stackoverflow.com/questions/49241264/\n",
    "# https://stackoverflow.com/questions/62863632/\n",
    "\n",
    "column_list = ['analysis_window', 't_symb']\n",
    "window_spec = Window.partitionBy([col(x) for x in column_list])\n",
    "sentiment_df = sentiment_df.withColumn('max_timestamp', spark_max(col('news_timestamp')).over(window_spec))\n",
    "\n",
    "# Convert from seconds from epoch to hour from epoch\n",
    "# Just divide the timestamp by 3600 seconds number of hours since epoch. \n",
    "# (Worry about taking floor later)\n",
    "sentiment_df = sentiment_df.withColumn('max_timestamp_hours', sentiment_df.max_timestamp / 3600)\n",
    "sentiment_df = sentiment_df.drop('max_timestamp')\n",
    "\n",
    "\n",
    "# 2. Convert news_timestamp which is seconds from epoch to hour from epoch.\n",
    "sentiment_df = sentiment_df.withColumn('news_timestamp_hours', sentiment_df.news_timestamp/3600)\n",
    "sentiment_df = sentiment_df.drop('news_timestamp')\n",
    "\n",
    "# 3. Calculate the number of hour difference between the current row value and max value in terms of \n",
    "#    news_timestamp hours from epoch.\n",
    "\n",
    "sentiment_df = sentiment_df.withColumn('diff_hours', sentiment_df.max_timestamp_hours - sentiment_df.news_timestamp_hours)\n",
    "sentiment_df = sentiment_df.drop('news_timestamp_hours') # don't need it anymore\n",
    "\n",
    "# 4. Divide this difference by 3 and get the floor\n",
    "staleness_period = 3\n",
    "sentiment_df = sentiment_df.withColumn('weight_denom_power', spark_floor(sentiment_df.diff_hours / staleness_period))\n",
    "sentiment_df = sentiment_df.drop('diff_hours')\n",
    "\n",
    "# Check for when difference is negative and throw and error or log it because something is wrong.\n",
    "# TODO maybe add this number to log file or throw error\n",
    "num_negatives = sentiment_df.filter(sentiment_df.weight_denom_power < 0).count()\n",
    "\n",
    "# 5. Calculate the weight which is 1/(2^h) where h = floor(diff_hour/3), h was calculated in step (3)\n",
    "sentiment_df = sentiment_df.withColumn('score_weight', 1/(2**sentiment_df.weight_denom_power))\n",
    "sentiment_df = sentiment_df.drop('weight_denom_power')\n",
    "\n",
    "# 6. Multiply the sentiment score by the weight to get the new time weighted score column\n",
    "sentiment_df = sentiment_df.withColumn('weighted_score', sentiment_df.score_weight * sentiment_df.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+--------------------+--------+-----------+--------------------+------------+-------------------+\n",
      "|   analysis_window|t_symb|          news_title|   label|      score| max_timestamp_hours|score_weight|     weighted_score|\n",
      "+------------------+------+--------------------+--------+-----------+--------------------+------------+-------------------+\n",
      "|2020-07-15_Hour=17|  GOOG|Lawsuits allege M...|NEGATIVE|-0.99602497|443016.3488888888...|         0.5|-0.4980124831199646|\n",
      "|2020-07-15_Hour=17|  GOOG|Gmail for G Suite...|POSITIVE| 0.99527836|443016.3488888888...|         0.5|0.49763917922973633|\n",
      "|2020-07-15_Hour=17|  GOOG|After $20 Billion...|NEGATIVE| -0.9974083|443016.3488888888...|         1.0|-0.9974082708358765|\n",
      "|2020-07-15_Hour=17|  GOOG|Amazon Extends Wo...|NEGATIVE| -0.9492833|443016.3488888888...|         1.0|-0.9492833018302917|\n",
      "|2020-07-15_Hour=17|  GOOG|Apple Ruling Make...|NEGATIVE|-0.99890846|443016.3488888888...|         1.0|-0.9989084601402283|\n",
      "+------------------+------+--------------------+--------+-----------+--------------------+------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted average scores and change the labels\n",
    "\n",
    "First we will sum all the weighted scores and divide it by the sum of the score weights i.e. get a weighted average. This operation will be on rows grouped by their respective analysis window and stock ticker symbol.\n",
    "\n",
    "Then, instead of just **positive** and **negative**, we want one more label called **uncertain** which is for scores less than 0.7 for either positive or negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sum of weights and sum of weighted scores\n",
    "sentiment_df = (sentiment_df.groupBy('analysis_window', 't_symb')\n",
    "                            .agg(spark_sum('weighted_score').alias('sum_scores'),\n",
    "                                 spark_sum('score_weight').alias('sum_weights'))\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- analysis_window: string (nullable = true)\n",
      " |-- t_symb: string (nullable = true)\n",
      " |-- sum_scores: double (nullable = true)\n",
      " |-- sum_weights: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final score for each stock\n",
    "sentiment_df = sentiment_df.withColumn('final_score', sentiment_df.sum_scores / sentiment_df.sum_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+-------------------+-----------------+--------------------+\n",
      "|   analysis_window|t_symb|         sum_scores|      sum_weights|         final_score|\n",
      "+------------------+------+-------------------+-----------------+--------------------+\n",
      "|2020-07-15_Hour=17|  GOOG| -6.029178576936829|   10.82861328125| -0.5567821493243732|\n",
      "|2020-07-15_Hour=17|  AAPL| -5.456488942727447|         13.15625|-0.41474500277263254|\n",
      "|2020-07-15_Hour=17|    FB| -3.147122867685539|5.354244232177734| -0.5877809698653788|\n",
      "|2020-07-15_Hour=17|  NFLX|-5.9542470138840145|   9.009521484375| -0.6608838243196739|\n",
      "|2020-07-15_Hour=17|  AMZN|  -5.34244602243416|      11.93359375| -0.4476812378864632|\n",
      "+------------------+------+-------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = (sentiment_df.withColumn('label',when(sentiment_df.final_score >= 0.5, 'POSITIVE')\n",
    "                                               .when(sentiment_df.final_score <= -0.5, 'NEGATIVE')\n",
    "                                               .otherwise('UNCERTAIN'))\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+---------+--------------------+\n",
      "|   analysis_window|t_symb|    label|         final_score|\n",
      "+------------------+------+---------+--------------------+\n",
      "|2020-07-15_Hour=17|  GOOG| NEGATIVE| -0.5567821493243732|\n",
      "|2020-07-15_Hour=17|  AAPL|UNCERTAIN|-0.41474500277263254|\n",
      "|2020-07-15_Hour=17|    FB| NEGATIVE| -0.5877809698653788|\n",
      "|2020-07-15_Hour=17|  NFLX| NEGATIVE| -0.6608838243196739|\n",
      "|2020-07-15_Hour=17|  AMZN|UNCERTAIN| -0.4476812378864632|\n",
      "+------------------+------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_df.select('analysis_window', 't_symb', 'label', 'final_score').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the results in DynamoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DynamoDB only accepts Decimal objects for floating point numbers when using boto3, so we need to convert the final score column from float to Decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only entries that we need for the website\n",
    "sentiment_df = sentiment_df.select('analysis_window', 't_symb', 'label', 'final_score')\n",
    "\n",
    "# Cast float to Decimal\n",
    "# precision: the maximum total number of digits (default: 10)\n",
    "# scale: the number of digits on right side of dot. (default: 0)\n",
    "sentiment_df = sentiment_df.withColumn('final_score', sentiment_df.final_score.cast(DecimalType(precision=10, scale=8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- analysis_window: string (nullable = true)\n",
      " |-- t_symb: string (nullable = true)\n",
      " |-- label: string (nullable = false)\n",
      " |-- final_score: decimal(10,8) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sentiment_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = [row.asDict() for row in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = dynamodb.Table('test_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use batch writer to automatically handle buffering and sending items in batches\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/guide/dynamodb.html\n",
    "with results_table.batch_writer() as batch:\n",
    "    for rd in results_dict:\n",
    "        batch.put_item(\n",
    "            Item=rd\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faangs_venv",
   "language": "python",
   "name": "faangs_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
